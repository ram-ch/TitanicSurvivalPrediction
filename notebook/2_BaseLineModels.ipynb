{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.dpi'] = 125\n",
    "\n",
    "# machine learning\n",
    "from sklearn.model_selection import KFold,cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import *\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (891, 9)\n",
      "y shape: (891,)\n"
     ]
    }
   ],
   "source": [
    "train_df=pd.read_csv('train_clean.csv')\n",
    "test_df=pd.read_csv('test_clean.csv')\n",
    "y=train_df.pop(\"Survived\").to_numpy()\n",
    "X=train_df.to_numpy()\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search on multiple models\n",
    "# Initialze the estimators\n",
    "\n",
    "knn=KNeighborsClassifier()\n",
    "mnb=MultinomialNB()\n",
    "lr=LogisticRegression(random_state=42)\n",
    "sgd=SGDClassifier(random_state=42,loss='log')\n",
    "svc=SVC(probability=True, random_state=42)\n",
    "dt=DecisionTreeClassifier(random_state=42)\n",
    "rf=RandomForestClassifier(random_state=42)\n",
    "gb=GradientBoostingClassifier(random_state=42)\n",
    "mlp=MLPClassifier(random_state=42)\n",
    "xgb_cl=xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "\n",
    "estimators={\"K Nearest neighbors\":knn,\n",
    "            \"Multi Nomial Naive Bayes\":mnb,\n",
    "            \"Logitic Regression\":lr,\n",
    "            \"SGD\":sgd,\n",
    "            \"Support Vectors\":svc,\n",
    "            \"Decision Trees\":dt,\n",
    "            \"Random Forests\":rf,\n",
    "            \"Gradient Boosting\":gb,\n",
    "            \"Neural Network\":mlp,\n",
    "            \"XGB\":xgb_cl}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:1, Train set: 801, Test set:90\n",
      "Fold:2, Train set: 802, Test set:89\n",
      "Fold:3, Train set: 802, Test set:89\n",
      "Fold:4, Train set: 802, Test set:89\n",
      "Fold:5, Train set: 802, Test set:89\n",
      "Fold:6, Train set: 802, Test set:89\n",
      "Fold:7, Train set: 802, Test set:89\n",
      "Fold:8, Train set: 802, Test set:89\n",
      "Fold:9, Train set: 802, Test set:89\n",
      "Fold:10, Train set: 802, Test set:89\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=10,random_state=42).split(X, y)\n",
    "\n",
    "i = 1\n",
    "# split()  method generate indices to split data into training and test set.\n",
    "for train_index, test_index in kfold:\n",
    "    print(f'Fold:{i}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Estimator</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>logloss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.816005</td>\n",
       "      <td>0.425819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Neural Network</td>\n",
       "      <td>0.795755</td>\n",
       "      <td>0.465074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Support Vectors</td>\n",
       "      <td>0.802459</td>\n",
       "      <td>0.479043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGB</td>\n",
       "      <td>0.816005</td>\n",
       "      <td>0.482913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Logitic Regression</td>\n",
       "      <td>0.793508</td>\n",
       "      <td>0.496929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Multi Nomial Naive Bayes</td>\n",
       "      <td>0.699338</td>\n",
       "      <td>0.685384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forests</td>\n",
       "      <td>0.814869</td>\n",
       "      <td>0.848340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SGD</td>\n",
       "      <td>0.774569</td>\n",
       "      <td>1.897962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>K Nearest neighbors</td>\n",
       "      <td>0.800325</td>\n",
       "      <td>1.898817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decision Trees</td>\n",
       "      <td>0.798040</td>\n",
       "      <td>3.279904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Estimator  accuracy   logloss\n",
       "0         Gradient Boosting  0.816005  0.425819\n",
       "6            Neural Network  0.795755  0.465074\n",
       "3           Support Vectors  0.802459  0.479043\n",
       "1                       XGB  0.816005  0.482913\n",
       "7        Logitic Regression  0.793508  0.496929\n",
       "9  Multi Nomial Naive Bayes  0.699338  0.685384\n",
       "2            Random Forests  0.814869  0.848340\n",
       "8                       SGD  0.774569  1.897962\n",
       "4       K Nearest neighbors  0.800325  1.898817\n",
       "5            Decision Trees  0.798040  3.279904"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fit_multi(X,y,metric):\n",
    "    results=pd.DataFrame(columns=[\"Estimator\",metric])\n",
    "    for est in estimators.items():      \n",
    "        kfold = KFold(n_splits=10,random_state=42).split(X, y)\n",
    "        if metric==\"accuracy\":             \n",
    "            score = cross_val_score(est[1], X, y, cv= kfold, scoring=\"accuracy\",n_jobs=-1)\n",
    "            vals=[est[0],score.mean()]                    \n",
    "        elif metric==\"logloss\":   \n",
    "            score = cross_val_score(est[1], X, y, cv= kfold, scoring=\"neg_log_loss\",n_jobs=-1)       \n",
    "            vals=[est[0],-score.mean()]\n",
    "        results.loc[len(results)]=vals        \n",
    "        results.sort_values(by=metric,ascending=False, inplace=True)\n",
    "    return results\n",
    "\n",
    "# refitting multiple models\n",
    "accuracy_results=fit_multi(X,y,metric='accuracy')\n",
    "logloss_results=fit_multi(X,y,metric='logloss')\n",
    "results=accuracy_results.merge(logloss_results, on='Estimator')\n",
    "results.sort_values(by='logloss',ascending=True, inplace=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gradient Boosting', 'Neural Network', 'Support Vectors', 'XGB', 'Logitic Regression']\n"
     ]
    }
   ],
   "source": [
    "# Sorting the log loss in desc and choosing the top 5 models for tuning\n",
    "print(list(results.Estimator[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_confusion_matrix(gs, X_test, y_test,cmap='Blues')  \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_roc_curve(gs, X_test, y_test,response_method='predict_proba')  \n",
    "# plt.plot([0, 1], [0, 1], color=\"red\", lw=1, linestyle=\"--\")\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel(\"False Positive Rate\")\n",
    "# plt.ylabel(\"True Positive Rate\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
